# scrapedPoGo

A comprehensive web scraper for Pokémon GO event data from [LeekDuck.com](https://leekduck.com).

## Table of Contents

- [Quick Start](#quick-start)
- [API Endpoints](#api-endpoints)
- [Data Quality Metrics](#data-quality-metrics)
- [JSON Schemas](#json-schemas)
- [Project Structure](#project-structure)
- [Scrapers](#scrapers)
- [Development](#development)
- [License](#license)

---

## Quick Start

### Installation

```bash
npm install
```

### Running Scrapers

```bash
# Scrape all basic data (events, raids, research, eggs, rocket lineups)
npm run scrape

# Scrape shiny Pokemon data
npm run scrapeshinies

# Scrape detailed event information
npm run detailedscrape

# Combine detailed data with basic events (also generates per-eventType files)
npm run combinedetails

# Generate data quality metrics
npm run metrics:generate
```

### Complete Workflow

For the most comprehensive event data:

```bash
npm run scrape              # Get basic event data
npm run detailedscrape      # Get detailed event info
npm run combinedetails      # Merge details + generate per-eventType files
```

### Output

All scraped data is saved to the `data/` directory as JSON files:
- `.json` - Formatted for readability
- `.min.json` - Minified for production use

---

## API Endpoints

All data is served from **https://pokemn.quest** and updated every 8 hours via automated scraping.

### Events
- Formatted: `https://pokemn.quest/data/events.json`
- Minimized: `https://pokemn.quest/data/events.min.json`
- [Documentation](docs/Events.md)

#### Event Types (per-type files)

Each `eventType` has its own file containing only events of that type (automatically generated by `combinedetails.js`):

- Formatted: `https://pokemn.quest/data/eventTypes/<eventType>.json`
- Minimized: `https://pokemn.quest/data/eventTypes/<eventType>.min.json`

### Raids
- Formatted: `https://pokemn.quest/data/raids.json`
- Minimized: `https://pokemn.quest/data/raids.min.json`
- [Documentation](docs/Raids.md)

### Research
- Formatted: `https://pokemn.quest/data/research.json`
- Minimized: `https://pokemn.quest/data/research.min.json`
- [Documentation](docs/Research.md)

### Eggs
- Formatted: `https://pokemn.quest/data/eggs.json`
- Minimized: `https://pokemn.quest/data/eggs.min.json`
- [Documentation](docs/Eggs.md)

### Rocket Lineups
- Formatted: `https://pokemn.quest/data/rocketLineups.json`
- Minimized: `https://pokemn.quest/data/rocketLineups.min.json`
- [Documentation](docs/RocketLineups.md)

### Shinies
- Formatted: `https://pokemn.quest/data/shinies.json`
- Minimized: `https://pokemn.quest/data/shinies.min.json`
- [Documentation](docs/Shinies.md)

---

## Data Quality Metrics

Monitor the health and completeness of scraped data with the built-in metrics dashboard.

### Metrics Dashboard

Access the live dashboard at: **https://pokemn.quest/web/metrics.html**

The dashboard provides:
- **System Overview**: Overall health status, total records, average completeness
- **Dataset Health Cards**: Individual status for each dataset with completeness bars
- **Issue Detection**: Automatic identification of missing fields, invalid data, and other problems
- **Auto-Refresh**: Updates every 5 minutes to show current data quality

### Metrics API

Metrics are available as JSON endpoints:
- Formatted: `https://pokemn.quest/data/metrics.json`
- Minimized: `https://pokemn.quest/data/metrics.min.json`

### Generate Metrics Locally

```bash
npm run metrics:generate
```

This analyzes all data files and generates:
- Completeness percentages for each dataset
- Issue detection (missing required fields, invalid dates, empty arrays)
- Health status classification (healthy/degraded/critical)
- Last updated timestamps

The metrics are automatically generated during the CI/CD pipeline after each scraping run.

---

## JSON Schemas

All data files have JSON Schema definitions for validation and type generation. Schemas are available in the [`schemas/`](schemas/) directory.

### Available Schemas

- [eggs.schema.json](schemas/eggs.schema.json) - Egg hatch data
- [events.schema.json](schemas/events.schema.json) - Events data  
- [raids.schema.json](schemas/raids.schema.json) - Raid boss data
- [research.schema.json](schemas/research.schema.json) - Field research data
- [rocketLineups.schema.json](schemas/rocketLineups.schema.json) - Team GO Rocket lineups
- [shinies.schema.json](schemas/shinies.schema.json) - Shiny availability data

### Validation

Validate all data files against their schemas:

```bash
npm run validate
```

### Image Size Analysis

Calculate the total size of images referenced in scraped data (what disk space they would consume if stored locally):

```bash
# Basic summary
npm run imagesize

# Verbose output with per-image details
npm run imagesize -- --verbose
```

This analyzes all image URLs in the data files and reports:
- Total unique images and their combined size
- Breakdown by CDN domain
- Size statistics (average, min, max)
- Individual file sizes (with `--verbose`)

### Usage

See the [schemas README](schemas/README.md) for:
- How to use schemas for validation in your application
- Generating TypeScript types from schemas
- Integration examples

---

## Project Structure

```
scrapedPoGo/
├── src/                    # Source code
│   ├── scrapers/           # Main scraper scripts
│   ├── pages/              # Page-specific scraper modules
│   │   └── detailed/       # Detailed event scrapers
│   └── utils/              # Utility functions
├── data/                   # Output data files (JSON)
├── docs/                   # API and data structure documentation
├── package.json            # Dependencies and scripts
└── README.md               # This file
```

### Source Code (`src/`)

#### Main Scrapers (`src/scrapers/`)

| File | Description |
|------|-------------|
| `scrape.js` | Primary scraper for basic data (events, raids, research, eggs, rocket lineups) |
| `detailedscrape.js` | Scraper for detailed event information |
| `combinedetails.js` | Combines detailed data with basic event data and generates per-eventType files |
| `scrapeShinies.js` | Scrapes shiny Pokemon availability data |
| `explore.js` | Utility for exploring page structure (development/debugging) |

#### Page Scrapers (`src/pages/`)

| File | Description |
|------|-------------|
| `events.js` | Scrapes event information from LeekDuck |
| `raids.js` | Scrapes raid boss data |
| `research.js` | Scrapes field research tasks and rewards |
| `eggs.js` | Scrapes egg hatch pool data |
| `rocketLineups.js` | Scrapes Team GO Rocket lineup data |
| `shinies.js` | Scrapes shiny Pokemon availability |

#### Detailed Event Scrapers (`src/pages/detailed/`)

All detailed scrapers use shared utilities from `scraperUtils.js` for consistent extraction and error handling.

**Core Event Types:**

| File | Event Type | Data Extracted |
|------|------------|----------------|
| `generic.js` | All events | Available sections detection |
| `event.js` | Generic events | Bonuses, spawns, custom sections |
| `communityday.js` | Community Day | Spawns, bonuses, featured attacks, photobombs, showcases, research |
| `spotlight.js` | Spotlight Hour | Featured Pokemon, bonus |
| `breakthrough.js` | Research Breakthrough | Reward Pokemon |

**Raid Event Types:**

| File | Event Type | Data Extracted |
|------|------------|----------------|
| `raidbattles.js` | Raid Battles | Bosses by tier, alternation patterns, featured attacks |
| `raidday.js` | Raid Day | Alternating bosses, ticket bonuses, special mechanics |
| `raidhour.js` | Raid Hour | Single featured boss |

**Research Event Types:**

| File | Event Type | Data Extracted |
|------|------------|----------------|
| `research.js` | Special/Masterwork Research | Tasks, promo codes, pricing, expiration |
| `timedresearch.js` | Timed Research | Tasks, rewards, availability window |

**Seasonal & Large Events:**

| File | Event Type | Data Extracted |
|------|------------|----------------|
| `season.js` | Seasons | Bonuses, eggs by tier, debuts, GO Pass |
| `gotour.js` | GO Tour | Habitats, raids, eggs, research, shiny debuts |

**Battle & Competitive:**

| File | Event Type | Data Extracted |
|------|------------|----------------|
| `gobattleleague.js` | GO Battle League | Leagues, CP caps, type restrictions |
| `teamgorocket.js` | Team GO Rocket | Leaders, Giovanni, shadow Pokemon |

**Max/Dynamax Events:**

| File | Event Type | Data Extracted |
|------|------------|----------------|
| `maxbattles.js` | Max Battles | Dynamax/Gigantamax Pokemon |
| `maxmondays.js` | Max Mondays | Weekly featured Dynamax |

**Other Event Types:**

| File | Event Type | Data Extracted |
|------|------------|----------------|
| `gopass.js` | GO Pass | Point tasks, rewards, milestone bonuses |
| `pokestopshowcase.js` | PokéStop Showcases | Featured Pokemon |

#### Utilities (`src/utils/`)

| File | Description |
|------|-------------|
| `shinyData.js` | Helper functions for shiny Pokemon data processing |
| `scraperUtils.js` | Shared scraper utilities (see below) |

**`scraperUtils.js` Functions:**

| Function | Description |
|----------|-------------|
| `writeTempFile()` | Write temporary JSON files for scraped data |
| `handleScraperError()` | Centralized error handling with backup fallback |
| `extractPokemonList()` | Extract Pokemon from `.pkmn-list-flex` elements |
| `getSectionHeaders()` | Discover all section headers on a page |
| `extractSection()` | Extract content (paragraphs, lists, pokemon) from a section |
| `extractBonuses()` | Extract bonus items and disclaimers |
| `extractRaidInfo()` | Extract raid boss information by tier |
| `extractResearchTasks()` | Extract special/timed/field research tasks |
| `extractEggPools()` | Extract egg hatches organized by distance tier |
| `extractPrice()` | Extract price information from text |
| `extractPromoCodes()` | Extract promo codes from page links |

---

## Scrapers

### Basic Scraping

```bash
npm run scrape
```

Generates basic data for all endpoints (events, raids, research, eggs, rocket lineups).

### Detailed Event Scraping

```bash
npm run detailedscrape
npm run combinedetails
```

Scrapes additional event-specific data and merges it into the events data. This includes:
- Featured attacks and move stats
- Photobomb Pokemon
- PokéStop Showcases
- Ticket pricing
- Promo codes
- Research task details
- Habitat spawns (GO Tour)
- And much more...

### Shiny Pokemon Data

```bash
npm run scrapeshinies
```

Generates shiny availability data used to augment `canBeShiny` fields in other endpoints.

---

## Development

### Technical Notes

- All scrapers use [JSDOM](https://github.com/jsdom/jsdom) for HTML parsing
- Data is fetched from LeekDuck.com with permission
- Temporary files during detailed scraping are stored in `data/temp/` and cleaned up automatically
- The project uses [moment.js](https://momentjs.com/) for date/time handling

### Adding a New Event Type Scraper

1. Create a new file in `src/pages/detailed/`
2. Import utilities from `../../utils/scraperUtils`
3. Export an async `get(url, id, bkp)` function
4. Use `writeTempFile()` to save data and `handleScraperError()` for error handling

Example:

```javascript
const { JSDOM } = require('jsdom');
const { writeTempFile, handleScraperError, extractPokemonList } = require('../../utils/scraperUtils');

async function get(url, id, bkp) {
    try {
        const dom = await JSDOM.fromURL(url, {});
        const doc = dom.window.document;
        
        const data = {
            featured: await extractPokemonList(doc.querySelector('.pkmn-list-flex'))
        };
        
        if (data.featured.length > 0) {
            writeTempFile(id, 'my-event-type', data);
        }
    } catch (err) {
        handleScraperError(err, id, 'my-event-type', bkp, 'myeventtype');
    }
}

module.exports = { get };
```

### NPM Scripts

| Script | Description |
|--------|-------------|
| `npm run scrape` | Run basic scraper (all data types) |
| `npm run scrapeshinies` | Run shiny Pokemon scraper |
| `npm run detailedscrape` | Scrape detailed event information |
| `npm run combinedetails` | Combine detailed data with basic events and generate per-eventType files |
| `npm run metrics:generate` | Generate data quality metrics dashboard |
| `npm run validate` | Validate all data files against JSON schemas |
| `npm run blob:upload` | Upload images to Vercel Blob Storage |
| `npm run blob:upload:dry` | Preview uploads without actually uploading |
| `npm run blob:upload:force` | Re-upload all images (overwrite existing) |

---

## Vercel Blob Storage

Images referenced in the scraped data can optionally be stored in [Vercel Blob](https://vercel.com/docs/storage/vercel-blob) for self-hosted image delivery.

### Setup

1. Create a Blob store in your Vercel project
2. Configure environment variables for Vercel Blob
3. Add `BLOB_READ_WRITE_TOKEN` to GitHub Actions secrets

### Upload Images

```bash
# Preview what would be uploaded
npm run blob:upload:dry

# Upload new images to Blob Storage
npm run blob:upload

# Force re-upload all images
npm run blob:upload:force
```

### Enable Blob URLs in Output

Set the `USE_BLOB_URLS=true` environment variable to transform image URLs in JSON output to use Vercel Blob storage URLs instead of external CDNs.

The URL mapping is stored in `data/blob-url-map.json` and automatically updated during uploads.

---

## Documentation

Detailed API documentation for each endpoint:

- [Events](docs/Events.md) - Event data with type-specific fields
- [Raids](docs/Raids.md) - Raid boss data with CP ranges
- [Research](docs/Research.md) - Field research tasks and rewards
- [Eggs](docs/Eggs.md) - Egg hatch pools by distance
- [Rocket Lineups](docs/RocketLineups.md) - Team GO Rocket lineups
- [Shinies](docs/Shinies.md) - Shiny Pokemon availability
- [Architecture Diagrams](docs/architecture-diagrams.md) - Mermaid system and flow diagrams

---

## License

MIT
